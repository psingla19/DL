Aim8 :- Use the appropriate dataset to realize multiple linear regression and show how various statistical tools like p-value, t-score, etc can be used for dimensionality reduction.


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


data = pd.read_csv('50_Startups.csv')
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values
print(X)


from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(),[3])], remainder ='passthrough' )
X = np.array(ct.fit_transform(X))
print(X)



from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=0)


from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train,y_train)
LinearRegression()


y_pred = regressor.predict(X_test)
np.set_printoptions(precision = 3)
print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1))))


import statsmodels.api as sm
X = np.append(arr = np.ones((50,1)).astype(int),values=X,axis=1)
X_opt =X[:,[0,1,2,3,4,5]]
X_opt = X_opt.astype(np.float64)
regressor_GLM = sm.GLM(endog=y,exog=X_opt).fit()
regressor_GLM.summary()



X_opt =X[:,[0,1,3,4,5]]
X_opt = X_opt.astype(np.float64)
regressor_GLM = sm.GLM(endog=y,exog=X_opt).fit()
regressor_GLM.summary()


X_opt =X[:,[0,3,4,5]]
X_opt = X_opt.astype(np.float64)
regressor_GLM = sm.GLM(endog=y,exog=X_opt).fit()
regressor_GLM.summary()


X_opt =X[:,[0,3,5]]
X_opt = X_opt.astype(np.float64)
regressor_GLM = sm.GLM(endog=y,exog=X_opt).fit()
regressor_GLM.summary()


